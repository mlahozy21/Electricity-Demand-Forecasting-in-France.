# -*- coding: utf-8 -*-
"""Data challenge Deep Learning Lahoz Marcos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N-qSj0k0Tr3vxqKWNJjoSlv5KogJK-8v

# Fichiers et bibliothèques

On importe les bibliothèques qu'on va utiliser et on lit les fichiers de données comme des data frames.
"""

import gc
gc.collect()
import pandas as pd
import numpy as np
import sklearn
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import date
import warnings
warnings.filterwarnings('ignore')
dftemps=pd.read_csv('train.csv')
print(dftemps.shape)
dfmeteo = pd.read_parquet('meteo.parquet')
print(dfmeteo.shape)

"""Avant commencer avec l'exploration des données on transforme les variables de date au type datetime pour travailler mieux aussi on les transformes pour ne pas s'inquiéter des changements horaires."""

dftemps['date'] = pd.to_datetime(dftemps['date'], utc=True)
#La fonction pd.to_datatime a en compte le changement d'heure
dftemps = dftemps.sort_values(by='date')

dfmeteo['date'] = pd.to_datetime(dfmeteo['date'], utc=True)
#La fonction pd.to_datatime a en compte le changement d'heure
dfmeteo = dfmeteo.sort_values(by='date')

"""# 1. Préparation des données et Visualisations

##Exploration des données

Ici on voit de quel data à quel date sont les données de consommation et les données météo.
"""

print(max(dftemps['date']))
print(min(dftemps['date']))

print(max(dfmeteo['date']))
print(min(dfmeteo['date']))

dftemps.columns

"""On va regarder quels sont les données manquantes dans les deux data frames."""

sns.heatmap(dftemps[['France', 'Auvergne-Rhône-Alpes', 'Bourgogne-Franche-Comté',
       'Bretagne', 'Centre-Val de Loire', 'Grand Est', 'Hauts-de-France',
       'Normandie', 'Nouvelle-Aquitaine', 'Occitanie', 'Pays de la Loire',
       "Provence-Alpes-Côte d'Azur", 'Île-de-France']].isnull())

sns.heatmap(dftemps[['Montpellier Méditerranée Métropole', 'Métropole Européenne de Lille',
       'Métropole Grenoble-Alpes-Métropole', "Métropole Nice Côte d'Azur",
       'Métropole Rennes Métropole', 'Métropole Rouen Normandie',
       "Métropole d'Aix-Marseille-Provence", 'Métropole de Lyon',
       'Métropole du Grand Nancy', 'Métropole du Grand Paris',
       'Nantes Métropole', 'Toulouse Métropole']].isnull())

"""On voit qu'il y a des données manquantes de consommation dans la première partie de 2017 pour plusieurs villes."""

9000/48 #J'ai fait ce calcul pour voir jusqu'à quel jour (plus ou moins) on a beaucoup des données manquantes pour les villes.
# Le neuf mille fait référence à l'index qu'on voit sur le heatmap et le 48 à qu'on a 48 lignes par jour (une chaque 30 min).

"""Ici on voit que beaucoup de variables météo sont inservibles parce qu'elles ont trop de NA. Après on verra cela avec plus de profondité."""

sns.heatmap(dfmeteo.isnull())

"""Les données météo sont pris en stations météo de plusieurs départements de la France (de 40 différents concrètement). En explorant un peu on voit qu'il n'y a qu'une seule station par département observée (sauf pour Nord, où il y en a deux)."""

for  dept in dfmeteo['nom_dept'].unique():
  print(dept)
  print(len(dfmeteo[dfmeteo['nom_dept']==dept]['code_epci'].unique()))

"""##Création des variables

### Elimination des variables pas importantes

On va prendre les variables qui ont moins d'un 5% de valeurs NULL.
"""

# Calculer le pourcentage de valeurs NaN par colonne
na_percentage = dfmeteo.isna().mean()

# Sélectionner les colonnes avec plus de 95% de valeurs NaN
cols_with_na = na_percentage[na_percentage > 0.05].index
print(cols_with_na)
print(len(cols_with_na))
# Filtrer le DataFrame uniquement avec ces colonnes
print(82-len(cols_with_na))
meteofrance=dfmeteo.drop(columns=cols_with_na).copy()

meteofrance.columns

"""Finalment je vais éliminer toutes les variables météo sauf la température parce que je n'ai pas pu améliorer mes modèles avec elles."""

meteofrance=meteofrance.drop(columns=['numer_sta','tend', 'cod_tend', 'dd', 'ff','td','u','pres','tend24','raf10', 'rafper', 'per', 'rr1', 'rr3', 'rr6', 'rr12',
       'rr24', 'coordonnees', 'nom', 'type_de_tendance_barometrique', 'tc',
       'latitude', 'longitude', 'altitude', 'libgeo', 'codegeo','nom_epci','code_epci','code_dep','code_reg','mois_de_l_annee'])

"""### Données par régions et villes

Je vais commencer avec la création d'une variable temperature pour chaque lieu qu'on doit prédire. Pour chaque ville j'ai pris la température de son département (je n'ai pas crée une variable pour Paris car sa région, Île de France, a des données d'un seul département, alors cela allais être un peu rédundant). Avec les régions et la France on a un problème parce qu' on a plusieurs départements en général. Donc, j'ai créé un dictionnaire avec chaque département et sa population, et un autre dictionnaire avec les départements de chaque région. Alors, pour créer leurs variables température j'ai fait une moyenne pondéré de la température de leurs départements correspondants selon la population. Jai fini avec un data frame avec les dates uniques (chaque demi heure) et des variables de température de chaque lieu.
"""

departaments = {
    'Manche': 495508, 'Aveyron': 275000, 'Var': 1095337, 'Côtes-d\'Armor': 605917, 'Hautes-Pyrénées': 230000,
    'Somme': 566252, 'Ille-et-Vilaine': 1098325, 'Marne': 565292, 'Bas-Rhin': 1152662, 'Landes': 422976,
    'Morbihan': 768687, 'Cher': 320000, 'Vienne': 439385, 'Charente-Maritime': 661404, 'Puy-de-Dôme': 662285,
    'Pyrénées-Orientales': 487307, 'Bouches-du-Rhône': 2056943, 'Loire-Atlantique': 1457806, 'Orne': 300000,
    'Gironde': 1654970, 'Nord': 2611293, 'Haut-Rhin': 767083, 'Lot': 170000, 'Hérault': 1201883,
    'Finistère': 921638, 'Aube': 300000, 'Seine-Maritime': 1255918, 'Drôme': 519458, 'Côte-d\'Or': 535503,
    'Meurthe-et-Moselle': 732486, 'Hautes-Alpes': 140000, 'Indre-et-Loire': 612160, 'Rhône': 1893692,
    'Essonne': 1313768, 'Haute-Loire': 220000, 'Alpes-Maritimes': 1103941, 'Calvados': 700633,
    'Ariège': 155000, 'Haute-Vienne': 371691, 'Haute-Garonne': 1434367
}

# Población total
population_totale = 32276570

# Calcular y mostrar el porcentaje de cada departamento
proportion = {dep: (population / population_totale) for dep, population in departaments.items()}

sum(proportion.values())

"""Je crée une fonction pour obtenir le pourcentage de la population qu'un département représent de la population totale (des départements des quels on a des données météo)."""

def obtenir_pourcentage(departament):
    if departament in departaments:
        population_departament = departaments[departament]
        pourcentage = (population_departament / population_totale)
        return pourcentage
    else:
        return "Departament pas trouvé."

"""Je crée un data frame avec les températures de chaque département. Aussi, on voit qu'il n'y a pas beaucoup des valeurs NULL sauf pour Var (mais cela ne sera pas important)."""

gardert = meteofrance.pivot(index='date', columns='nom_dept', values='t').copy()
gardert.columns

sns.heatmap(gardert[['Alpes-Maritimes', 'Ariège', 'Aube', 'Aveyron', 'Bas-Rhin',
       'Bouches-du-Rhône', 'Calvados', 'Charente-Maritime', 'Cher',
       "Côte-d'Or", "Côtes-d'Armor", 'Drôme', 'Essonne', 'Finistère',
       'Gironde', 'Haut-Rhin', 'Haute-Garonne', 'Haute-Loire', 'Haute-Vienne',
       'Hautes-Alpes', 'Hautes-Pyrénées', 'Hérault']].isnull())

sns.heatmap(gardert[['Ille-et-Vilaine',
       'Indre-et-Loire', 'Landes', 'Loire-Atlantique', 'Lot', 'Manche',
       'Marne', 'Meurthe-et-Moselle', 'Morbihan', 'Nord', 'Orne',
       'Puy-de-Dôme', 'Pyrénées-Orientales', 'Rhône', 'Seine-Maritime',
       'Somme', 'Var', 'Vienne']].isnull())

"""On agroupe les départements selon leurs régions pour obtenir après une variable température pour chaque région. Et on fait le même avec les villes et leur départements."""

regions = {
    "Auvergne-Rhône-Alpes": ["Rhône", "Drôme", "Puy-de-Dôme", "Haute-Loire"],
    "Bourgogne-Franche-Comté": ["Côte-d'Or"],
    "Bretagne": ["Ille-et-Vilaine", "Finistère", "Morbihan", "Côtes-d'Armor"],
    "Centre-Val de Loire": ["Cher", "Indre-et-Loire"],
    "Grand Est": ["Aube", "Haut-Rhin", "Bas-Rhin", "Marne", "Meurthe-et-Moselle"],
    "Hauts-de-France": ["Nord", "Somme"],
    "Normandie": ["Seine-Maritime", "Calvados", "Orne", "Manche"],
    "Nouvelle-Aquitaine": ["Gironde", "Haute-Vienne", "Vienne", "Landes", "Charente-Maritime"],
    "Occitanie": ["Haute-Garonne", "Pyrénées-Orientales", "Hautes-Pyrénées", "Lot", "Ariège", "Aveyron", "Hérault"],
    "Pays de la Loire": ["Loire-Atlantique"],
    "Provence-Alpes-Côte d'Azur": ["Bouches-du-Rhône", "Alpes-Maritimes", "Hautes-Alpes", "Var"],
    "Île-de-France": ["Essonne"]
}

depts = {
        "Hérault": 'Montpellier Méditerranée Métropole',
        "Nord": 'Métropole Européenne de Lille',
        "Alpes-Maritimes": "Métropole Nice Côte d'Azur",
        "Bouches-du-Rhône":"Métropole d'Aix-Marseille-Provence",
        "Rhône":"Métropole de Lyon", #et grenoble
        "Meurthe-et-Moselle":'Métropole du Grand Nancy',
        "Haute-Garonne":'Toulouse Métropole',
        "Ille-et-Vilaine":'Métropole Rennes Métropole',
        "Seine-Maritime": 'Métropole Rouen Normandie'
}

for region, departements in regions.items():
   # Filtrer les colonnes correspondant aux départements de la région
    df_region = gardert[departements]
    # Calculer les poids pour chaque département
    poids = df_region.columns.to_series().apply(obtenir_pourcentage)
   # Calculer la moyenne pondérée pour chaque ligne
    gardert[region] = df_region.apply(lambda row: (row * poids).sum() / poids.sum(), axis=1)

for dept, metropole in depts.items():
    gardert[metropole]=gardert[dept]

"""On garde le data frame gardert pour l'utiliser après."""

gardert.head()

"""Maintenant je définis une fonction pour calculer la moyenne pondéré de la variable température pour obtenir une estimation de la température de la France."""

def moyenne_pondéré_par_colonne(groupe):
    # Filtrer les colonnes numériques
    colonnes_numériques = ['t']
    résultats = {}

    # Pour chaque colonne numérique, calculez sa moyenne pondérée
    for col in colonnes_numériques:
        # Obtenez les poids de chaque ligne
        poids = groupe['nom_dept'].apply(obtenir_pourcentage)

        # Calculer la moyenne pondérée
        moyenne_pondéré = (groupe[col] * poids).sum() / poids.sum()
        résultats[col] = moyenne_pondéré
    return pd.Series(résultats)
#Regrouper par 'date' et appliquer la fonction de moyenne pondérée par colonne
résultat = meteofrance.groupby('date').apply(moyenne_pondéré_par_colonne).copy()
print(résultat)

"""Je garde le data frame résultat pour l'ajouter plus tard. Maintenant, j'élimine les dates répétées. Aussi je peux éliminer des variables comme 'nom_dept' et 'nom_reg' qu'il ne faut pas garder."""

dfm=meteofrance.copy()

dfm=dfm.drop([ 'nom_dept', 'nom_reg'], axis=1)

dfm = dfm.drop_duplicates(subset='date')

dfm=dfm.reset_index(drop=True)

dfm.head()

résultat=résultat.reset_index()

gardert=gardert.reset_index()

gardert.columns

"""Maintenant on peut ajouter les différentes variables de températures au data frame et mettre tout ensemble."""

dfm[['t-Auvergne-Rhône-Alpes',
       't-Bourgogne-Franche-Comté', 't-Bretagne', 't-Centre-Val de Loire',
       't-Grand Est', 't-Hauts-de-France', 't-Normandie', 't-Nouvelle-Aquitaine',
       't-Occitanie', 't-Pays de la Loire', "t-Provence-Alpes-Côte d'Azur",
       't-Île-de-France', 't-Montpellier Méditerranée Métropole',
       't-Métropole Européenne de Lille', "t-Métropole Nice Côte d'Azur", 't-Métropole Rennes Métropole', 't-Métropole Rouen Normandie',
       "t-Métropole d'Aix-Marseille-Provence",
       't-Métropole de Lyon',
       't-Métropole du Grand Nancy',
       't-Toulouse Métropole']]=gardert[['Auvergne-Rhône-Alpes',
       'Bourgogne-Franche-Comté', 'Bretagne', 'Centre-Val de Loire',
       'Grand Est', 'Hauts-de-France', 'Normandie', 'Nouvelle-Aquitaine',
       'Occitanie', 'Pays de la Loire', "Provence-Alpes-Côte d'Azur",
       'Île-de-France', 'Montpellier Méditerranée Métropole',
       'Métropole Européenne de Lille', "Métropole Nice Côte d'Azur", 'Métropole Rennes Métropole', 'Métropole Rouen Normandie',
       "Métropole d'Aix-Marseille-Provence",
       'Métropole de Lyon',
       'Métropole du Grand Nancy',
       'Toulouse Métropole']]

dfm['t']=résultat['t']

dfm.head()

"""### Interpolation pour créer des données toutes les 30 minutes

On applique une interpolation cubique de degré 3 pour créer des donnes chaque 30 min comme celles qu'on doit prédire.
"""

dfreindex=dfm.copy()

# Générer une série de fois toutes les 30 minutes
hora_nueva = pd.date_range(start=dfreindex['date'].min(), end=dfreindex['date'].max()+ pd.Timedelta(minutes=90), freq='30T')
# Réindexez le DataFrame avec la nouvelle série temporelle
dfreindex = dfreindex.set_index('date').reindex(hora_nueva)

# Interpoler les valeurs de température
df_interpolado = dfreindex.interpolate(method='cubic', order=3)

# Réinitialisez l'index pour que la colonne des heures redevienne une colonne normale
dfint = df_interpolado.rename(columns={'index': 'date'})

"""J'ai ajouté trois colonnes parce que les données a prèdire vont jusqu'à 22:30 le dernier jour. J'ai copié les données météo de ma dernière observation pour estimer ces données météo."""

dfint.tail(7)

dfint=dfint.reset_index()
dfint = dfint.rename(columns={'index': 'date'})

dfint.iloc[105163]=dfint.iloc[105162]
dfint.iloc[105164]=dfint.iloc[105162]
dfint.iloc[105165]=dfint.iloc[105162]
dfint.at[105163, 'date'] = dfint.at[dfint.index[105163], 'date'] + pd.Timedelta(minutes=30)
dfint.at[105164, 'date'] = dfint.at[dfint.index[105164], 'date'] + pd.Timedelta(minutes=60)
dfint.at[105165, 'date'] = dfint.at[dfint.index[105165], 'date'] + pd.Timedelta(minutes=90)

dfint.tail()

"""### Création des variables calendrier

On crée des variables classiques de calendrier comme l'heure, le jour de la semaine ou le mois. En plus, j'applique One-Hot-Encoding aux variables 'dayofweek' et 'month'.
"""

dfint['hour'] = dfint['date'].dt.hour+dfint['date'].dt.minute/60
dfint['dayofweek']= dfint['date'].dt.dayofweek
dfint['month']= dfint['date'].dt.month
dfint['year']= dfint['date'].dt.year

# Crear columnas binarias para cada valor único en la columna 'Color'
df_dummies = pd.get_dummies(dfint['dayofweek'], prefix='dayofweek')
dfint = pd.concat([dfint, df_dummies], axis=1)  # Añadir las nuevas columnas
df_dummies = pd.get_dummies(dfint['month'], prefix='month')
dfint = pd.concat([dfint, df_dummies], axis=1)  # Añadir las nuevas columnas

"""### Jours fériés

Je vais créer des variables relationnées avec les vacances ou des périodes d'inactivité.
"""

dfinactivité=dfint.copy()

fériés = [
    (2017, 1, 1), (2017, 4, 17), (2017, 5, 1), (2017, 5, 8), (2017, 5, 25),
    (2017, 6, 5), (2017, 7, 14), (2017, 8, 15), (2017, 11, 1), (2017, 11, 11),
    (2017, 12, 25), (2018, 1, 1), (2018, 4, 2), (2018, 5, 1), (2018, 5, 8),
    (2018, 5, 10), (2018, 5, 21), (2018, 7, 14), (2018, 8, 15), (2018, 11, 1),
    (2018, 11, 11), (2018, 12, 25), (2019, 1, 1), (2019, 4, 22), (2019, 5, 1),
    (2019, 5, 8), (2019, 5, 30), (2019, 6, 10), (2019, 7, 14), (2019, 8, 15),
    (2019, 11, 1), (2019, 11, 11), (2019, 12, 25), (2020, 1, 1), (2020, 4, 13),
    (2020, 5, 1), (2020, 5, 8), (2020, 5, 21), (2020, 6, 1), (2020, 7, 14),
    (2020, 8, 15), (2020, 11, 1), (2020, 11, 11), (2020, 12, 25), (2021, 1, 1),
    (2021, 4, 5), (2021, 5, 1), (2021, 5, 8), (2021, 5, 13), (2021, 5, 24),
    (2021, 7, 14), (2021, 8, 15), (2021, 11, 1), (2021, 11, 11), (2021, 12, 25),
    (2022, 1, 1), (2022, 4, 18), (2022, 5, 1), (2022, 5, 8), (2022, 5, 26),
    (2022, 6, 6), (2022, 7, 14), (2022, 8, 15), (2022, 11, 1), (2022, 11, 11),
    (2022, 12, 25), (2023, 1, 1)
]
fériés = pd.to_datetime([f'{année}-{mois:02d}-{jour:02d}' for année, mois, jour in fériés])
fériés=pd.DataFrame(fériés,columns=['date'])
preuve=dfinactivité['date'].dt.tz_localize(None).copy()
preuve=pd.to_datetime(preuve.dt.date)
fériés['date']=pd.to_datetime(fériés['date'])
dfinactivité['is_holiday']=preuve.isin(fériés['date'])

dates = pd.date_range(start="2017-07-07", end="2017-09-08", freq="D")
dates1 = pd.date_range(start="2018-07-07", end="2018-09-02", freq="D")
dates2 = pd.date_range(start="2019-07-06", end="2019-09-01", freq="D")
dates3 = pd.date_range(start="2020-07-04", end="2020-08-31", freq="D")
dates4 = pd.date_range(start="2021-07-06", end="2021-08-31", freq="D")
dates5 = pd.date_range(start="2022-07-07", end="2022-08-31", freq="D")
dates = pd.Series(dates)
dates1 = pd.Series(dates1)
dates2 = pd.Series(dates2)
dates3 = pd.Series(dates3)
dates4 = pd.Series(dates4)
dates5 = pd.Series(dates5)
vac = pd.concat([dates, dates1, dates2, dates3, dates4, dates5])
vac=vac.reset_index(drop=True)

vac=pd.DataFrame(vac,columns=['date'])
vac['date']=pd.to_datetime(vac['date'])
dfinactivité['summer_break'] = preuve.isin(vac['date'])

dates = pd.date_range(start="2017-12-23", end="2018-01-07", freq="D")
dates1 = pd.date_range(start="2018-12-22", end="2019-01-06", freq="D")
dates2 = pd.date_range(start="2019-12-21", end="2020-01-05", freq="D")
dates3 = pd.date_range(start="2020-12-19", end="2021-01-03", freq="D")
dates4 = pd.date_range(start="2021-12-18", end="2022-01-02", freq="D")
dates5 = pd.date_range(start="2022-12-19", end="2022-12-31", freq="D")
dates = pd.Series(dates)
dates1 = pd.Series(dates1)
dates2 = pd.Series(dates2)
dates3 = pd.Series(dates3)
dates4 = pd.Series(dates4)
dates5 = pd.Series(dates5)
vac = pd.concat([dates, dates1, dates2, dates3, dates4, dates5])
vac=vac.reset_index(drop=True)
# Convertimos las fechas al formato (año, mes, día)
vac=pd.DataFrame(vac,columns=['date'])
vac['date']=pd.to_datetime(vac['date'])
dfinactivité['winter_break'] = preuve.isin(vac['date'])

#Dates de vacances pour la zone A entre 2017 et 2022
vacancesA = {
    2017: [
        ('2017-07-06', '2017-09-03'),
        ('2017-10-21', '2017-11-06'),
        ('2017-12-23', '2017-12-31'),
        ('2017-02-11', '2017-02-27'),
        ('2017-04-08', '2017-04-24')
    ],

    2018: [
        ('2018-01-01', '2018-01-07'),
        ('2018-07-06', '2018-09-03'),
        ('2018-10-20', '2018-11-05'),
        ('2018-12-22', '2018-12-31'),
        ('2018-02-10', '2018-02-26'),
        ('2018-04-07', '2018-04-23')
    ],

    2019: [
        ('2019-01-01', '2019-01-06'),
        ('2019-07-05', '2019-09-02'),
        ('2019-10-19', '2019-11-04'),
        ('2019-12-22', '2019-12-31'),
        ('2019-02-16', '2019-03-04'),
        ('2019-04-13', '2019-04-29')
    ],

    2020: [
        ('2020-01-01', '2020-01-06'),
        ('2020-07-04', '2020-09-01'),
        ('2020-10-17', '2020-11-02'),
        ('2020-12-19', '2020-12-31'),
        ('2020-02-22', '2020-03-09'),
        ('2020-04-18', '2020-05-04')
    ],

    2021: [
        ('2021-01-01', '2021-01-04'),
        ('2021-07-06', '2021-09-02'),
        ('2021-10-23', '2021-11-08'),
        ('2021-12-18', '2021-12-31'),
        ('2021-02-06', '2021-02-22'),
        ('2021-04-10', '2021-04-26')
    ],

    2022: [
        ('2022-01-01', '2022-01-03'),
        ('2022-07-07', '2022-09-01'),
        ('2022-10-22', '2022-11-07'),
        ('2022-12-17', '2022-12-31'),
        ('2022-02-12', '2022-02-28'),
        ('2022-04-16', '2022-05-02')
    ]
}

# Créez un DataFrame avec des dates de 2017 à 2022
dates = pd.date_range(start='2017-01-01', end='2022-12-31', freq='D')

df = pd.DataFrame({'date': dates})

# On s'assure que les dates dans le DataFrame ne sont pas contaminée par le fuseau horaire
df['date'] = df['date'].dt.tz_localize(None)

# Créez un ensemble de dates de vacances pour chaque année
fériés_par_année = {}

for année, vacances in vacancesA.items():
    fériés = set()
    for start, end in vacances:
        dates_vacances = pd.date_range(start=start, end=end, freq='D')
        fériés.update(dates_vacances.date)  # Ajouter uniquement la partie date (sans l'heure)
    fériés_par_année[année] = fériés

# Fonction optimisée pour vérifier si une date est un jour férié
def est_vacances(date):
    return date.date() in fériés_par_année[date.year]

dfinactivité['vacances_scolairesA'] = dfinactivité['date'].apply(est_vacances)
dfinactivité['vacances_scolairesA']=dfinactivité['vacances_scolairesA'].astype(float)

#Dates de vacances pour la zone B entre 2017 et 2022
vacancesB = {
        2017: [
        ('2017-07-06', '2017-09-03'),
        ('2017-10-21', '2017-11-06'),
        ('2017-12-23', '2017-12-31'),
        ('2017-02-11', '2017-02-27'),
        ('2017-04-08', '2017-04-24')
    ],
        2018: [
        ('2018-01-01', '2018-01-07'),
        ('2018-07-06', '2018-09-03'),
        ('2018-10-20', '2018-11-05'),
        ('2018-12-22', '2018-12-31'),
        ('2018-02-24', '2018-03-12'),
        ('2018-04-21', '2018-05-07')
    ],
        2019: [
        ('2019-01-01', '2019-01-07'),
        ('2019-07-05', '2019-09-02'),
        ('2019-10-19', '2019-11-04'),
        ('2019-12-22', '2019-12-31'),
        ('2019-02-09', '2019-02-25'),
        ('2019-04-06', '2019-04-23')
    ],
        2020: [
        ('2020-01-01', '2020-01-06'),
        ('2020-07-04', '2020-09-01'),
        ('2020-10-17', '2020-11-02'),
        ('2020-12-19', '2020-12-31'),
        ('2020-02-15', '2020-03-02'),
        ('2020-04-11', '2020-04-27')
    ],
        2021: [
        ('2021-01-01', '2021-01-04'),
        ('2021-07-06', '2021-09-02'),
        ('2021-10-23', '2021-11-08'),
        ('2021-12-18', '2021-12-31'),
        ('2021-02-20', '2021-03-08'),
        ('2021-04-24', '2021-05-10')
    ],
        2022: [
        ('2022-01-01', '2022-01-03'),
        ('2022-07-07', '2022-09-01'),
        ('2022-10-22', '2022-11-07'),
        ('2022-12-17', '2022-12-31'),
        ('2022-02-05', '2022-02-21'),
        ('2022-04-09', '2022-04-25')
    ]
}
# Créez un DataFrame avec des dates de 2017 à 2022
dates = pd.date_range(start='2017-01-01', end='2022-12-31', freq='D')

df = pd.DataFrame({'date': dates})

# On s'assure que les dates dans le DataFrame ne sont pas contaminée par le fuseau horaire
df['date'] = df['date'].dt.tz_localize(None)

# Créez un ensemble de dates de vacances pour chaque année
fériés_par_année = {}

for année, vacances in vacancesB.items():
    fériés = set()
    for start, end in vacances:
        dates_vacances = pd.date_range(start=start, end=end, freq='D')
        fériés.update(dates_vacances.date)  # Ajouter uniquement la partie date (sans l'heure)
    fériés_par_année[année] = fériés

# Fonction optimisée pour vérifier si une date est un jour férié
def est_vacances(date):
    return date.date() in fériés_par_année[date.year]

dfinactivité['vacances_scolairesB'] = dfinactivité['date'].apply(est_vacances)
dfinactivité['vacances_scolairesB']=dfinactivité['vacances_scolairesB'].astype(float)

#Dates de vacances pour la zone C entre 2017 et 2022
vacancesC = {
        2017: [
        ('2017-07-06', '2017-09-03'),
        ('2017-10-21', '2017-11-06'),
        ('2017-12-23', '2017-12-31'),
        ('2017-02-11', '2017-02-27'),
        ('2017-04-08', '2017-04-24')
    ],
            2018: [
        ('2018-01-01', '2018-01-07'),
        ('2018-07-06', '2018-09-03'),
        ('2018-10-20', '2018-11-05'),
        ('2018-12-22', '2018-12-31'),
        ('2018-02-17', '2018-03-05'),
        ('2018-04-14', '2018-04-30')
    ],
            2019: [
        ('2019-01-01', '2019-01-07'),
        ('2019-07-05', '2019-09-02'),
        ('2019-10-19', '2019-11-04'),
        ('2019-12-22', '2019-12-31'),
        ('2019-02-23', '2019-03-11'),
        ('2019-04-20', '2019-05-06')
    ],
            2020: [
        ('2020-01-01', '2020-01-07'),
        ('2020-07-04', '2020-09-01'),
        ('2020-10-17', '2020-11-02'),
        ('2020-12-19', '2020-12-31'),
        ('2020-02-08', '2020-02-24'),
        ('2020-04-04', '2020-04-20')
    ],
            2021: [
        ('2021-01-01', '2021-01-04'),
        ('2021-07-06', '2021-09-02'),
        ('2021-10-23', '2021-11-08'),
        ('2021-12-18', '2021-12-31'),
        ('2021-02-13', '2021-03-01'),
        ('2021-04-17', '2021-05-03')
    ],
            2022: [
        ('2022-01-01', '2022-01-03'),
        ('2022-07-07', '2022-09-01'),
        ('2022-10-22', '2022-11-07'),
        ('2022-12-17', '2022-12-31'),
        ('2022-02-19', '2022-03-07'),
        ('2022-04-23', '2022-05-09')
    ]}
# Créez un DataFrame avec des dates de 2017 à 2022
dates = pd.date_range(start='2017-01-01', end='2022-12-31', freq='D')

df = pd.DataFrame({'date': dates})

# On s'assure que les dates dans le DataFrame ne sont pas contaminée par le fuseau horaire
df['date'] = df['date'].dt.tz_localize(None)

# Créez un ensemble de dates de vacances pour chaque année
fériés_par_année = {}

for année, vacances in vacancesC.items():
    fériés = set()
    for start, end in vacances:
        dates_vacances = pd.date_range(start=start, end=end, freq='D')
        fériés.update(dates_vacances.date)  # Ajouter uniquement la partie date (sans l'heure)
    fériés_par_année[année] = fériés

# Fonction optimisée pour vérifier si une date est un jour férié
def est_vacances(date):
    return date.date() in fériés_par_année[date.year]

dfinactivité['vacances_scolairesC'] = dfinactivité['date'].apply(est_vacances)
dfinactivité['vacances_scolairesC']=dfinactivité['vacances_scolairesC'].astype(float)

"""Je veux avoir comme des périodes exceptionnelle les deux confinements dû au covid en 2020."""

# Définir les dates de début et de fin des confinements
premier_confinament_début = '2020-03-17'
premier_confinament_fin = '2020-05-11'

deuxième_confinament_début = '2020-10-30'
deuxième_confinament_fin = '2020-12-15'

# Convertir des dates en objets datetime
premier_confinament_début = pd.to_datetime(premier_confinament_début)
premier_confinament_fin = pd.to_datetime(premier_confinament_fin)

deuxième_confinament_début = pd.to_datetime(deuxième_confinament_début)
deuxième_confinament_fin = pd.to_datetime(deuxième_confinament_fin)

# Créer des listes de jours pour chaque confinement
premier_confinament_jours = pd.date_range(premier_confinament_début, premier_confinament_fin).tolist()
deuxième_confinament_jours = pd.date_range(deuxième_confinament_début, deuxième_confinament_fin).tolist()

tous_les_jours_confinement1 = premier_confinament_jours
tous_les_jours_confinement2 = deuxième_confinament_jours
tous_les_jours_confinement1 = pd.to_datetime(tous_les_jours_confinement1)
tous_les_jours_confinement2 = pd.to_datetime(tous_les_jours_confinement2)
tous_les_jours_confinement1=tous_les_jours_confinement1.date
tous_les_jours_confinement2=tous_les_jours_confinement2.date

dfinactivité['confinement1']=0
for i in range(1,len(dfinactivité['date'])):
  if dfinactivité['date'].iloc[i].date() in tous_les_jours_confinement1:
    dfinactivité['confinement1'].iloc[i]=1

dfinactivité['confinement2']=0
for i in range(1,len(dfinactivité['date'])):
  if dfinactivité['date'].iloc[i].date() in tous_les_jours_confinement2:
    dfinactivité['confinement2'].iloc[i]=1

dfinactivité['confinement1'].unique()

"""### Économie

J'avais fait des variables liées à l'économie, cependant je ne les ai pas trouvé intéressantes pour mes modèles.
"""

dfeco=dfinactivité.copy()

"""L'indice PMI est considéré comme un indicateur précurseur de la croissance économique et peut être utilisé pour prédire les tendances à venir. Voici comment il fonctionne : L'indice est basé sur des enquêtes mensuelles auprès des entreprises dans les secteurs de la fabrication et des services."""

# Dictionnaire avec les valeurs PMI industrielles pour chaque combinaison d'année et de mois
pmiman_dict = {
    (2022, 12): 48.3, (2022, 11): 47.2, (2022, 10): 47.7, (2022, 9): 50.6,
    (2022, 8): 49.5, (2022, 7): 51.4, (2022, 6): 54.6, (2022, 5): 55.7,
    (2022, 4): 54.7, (2022, 3): 57.2, (2022, 2): 55.5, (2022, 1): 55.6,
    (2021, 12): 55.9, (2021, 11): 53.6, (2021, 10): 55.0, (2021, 9): 57.5,
    (2021, 8): 58.0, (2021, 7): 59.0, (2021, 6): 59.4, (2021, 5): 59.2,
    (2021, 4): 59.3, (2021, 3): 56.1, (2021, 2): 51.6, (2021, 1): 51.1,
    (2020, 12): 49.6, (2020, 11): 51.3, (2020, 10): 51.2, (2020, 9): 49.8,
    (2020, 8): 52.4, (2020, 7): 52.3, (2020, 6): 40.6, (2020, 5): 31.5,
    (2020, 4): 43.2, (2020, 3): 49.8, (2020, 2): 51.1, (2020, 1): 50.4,
    (2019, 12): 50.9, (2019, 11): 51.7, (2019, 10): 51.1, (2019, 9): 51.0,
    (2019, 8): 50.5, (2019, 7): 50.9, (2019, 6): 51.2, (2019, 5): 51.8,
    (2019, 4): 52.7, (2019, 3): 52.0, (2019, 2): 52.3, (2019, 1): 52.3,
    (2018, 12): 51.4, (2018, 11): 52.0, (2018, 10): 53.2, (2018, 9): 54.1,
    (2018, 8): 54.2, (2018, 7): 54.9, (2018, 6): 55.0, (2018, 5): 54.6,
    (2018, 4): 54.7, (2018, 3): 54.5, (2018, 2): 55.3, (2018, 1): 55.6,
    (2017, 12): 55.4, (2017, 11): 55.2, (2017, 10): 55.5, (2017, 9): 55.0,
    (2017, 8): 52.4, (2017, 7): 51.5,
    (2017, 6): 51.6, (2017, 5): 52.0, (2017, 4): 52.7,
    (2017, 3): 53.2, (2017, 2): 53.4, (2017, 1): 52.9
}

dfeco['pmind'] = dfeco.apply(lambda row: pmiman_dict.get((row['year'], row['month']), None), axis=1)

# Dictionnaire avec les valeurs PMI de services pour chaque combinaison d'année et de mois
pmi_ser = {
    (2022, 12): 49.3, (2022, 11): 49.4, (2022, 10): 51.7,
    (2022, 9): 52.9, (2022, 8): 51.2, (2022, 7): 53.2,
    (2022, 6): 54.4, (2022, 5): 58.4, (2022, 4): 58.9,
    (2022, 3): 58.8, (2022, 2): 57.4, (2022, 1): 57.4,

    (2021, 12): 57.1, (2021, 11): 58.2, (2021, 10): 56.6,
    (2021, 9): 56.2, (2021, 8): 56.3, (2021, 7): 56.8,
    (2021, 6): 57.8, (2021, 5): 56.6, (2021, 4): 56.6,
    (2021, 3): 50.3, (2021, 2): 50.4, (2021, 1): 48.2,

    (2020, 12): 49.1, (2020, 11): 38.8, (2020, 10): 46.5,
    (2020, 9): 47.5, (2020, 8): 51.5, (2020, 7): 57.3,
    (2020, 6): 50.7, (2020, 5): 31.1, (2020, 4): 10.2,
    (2020, 3): 27.4, (2020, 2): 52.5, (2020, 1): 52.6,

    (2019, 12): 52.4, (2019, 11): 52.4, (2019, 10): 52.2,
    (2019, 9): 52.9, (2019, 8): 52.9, (2019, 7): 53.4,
    (2019, 6): 53.3, (2019, 5): 52.6, (2019, 4): 52.2,
    (2019, 3): 52.9, (2019, 2): 53.1, (2019, 1): 51.5,

    (2018, 12): 51.7, (2018, 11): 50.5, (2018, 10): 50.5,
    (2018, 9): 51.5, (2018, 8): 52.6, (2018, 7): 52.2,
    (2018, 6): 52.7, (2018, 5): 53.1, (2018, 4): 54.0,
    (2018, 3): 55.0, (2018, 2): 54.0, (2018, 1): 53.8,

    (2017, 12): 53.0, (2017, 11): 52.8, (2017, 10): 51.6,
    (2017, 9): 51.8, (2017, 8): 52.4, (2017, 7): 51.5,
    (2017, 6): 51.6, (2017, 5): 52.0, (2017, 4): 52.7,
    (2017, 3): 53.2, (2017, 2): 53.4, (2017, 1): 52.9
}
dfeco['pmiser'] = dfeco.apply(lambda row: pmi_ser.get((row['year'], row['month']), None), axis=1)

taux_chômage = {
    # 2017
    (2017, 1): 9.4, (2017, 2): 9.4, (2017, 3): 9.4,  # Taux du T1
    (2017, 4): 9.2, (2017, 5): 9.2, (2017, 6): 9.2,  # Taux du T2
    (2017, 7): 9.1, (2017, 8): 9.1, (2017, 9): 9.1,  # Taux du T3
    (2017, 10): 9.0, (2017, 11): 9.0, (2017, 12): 9.0,  # Taux du T4

    # 2018
    (2018, 1): 8.9, (2018, 2): 8.9, (2018, 3): 8.9,
    (2018, 4): 8.8, (2018, 5): 8.8, (2018, 6): 8.8,
    (2018, 7): 8.7, (2018, 8): 8.7, (2018, 9): 8.7,
    (2018, 10): 8.6, (2018, 11): 8.6, (2018, 12): 8.6,

    # 2019
    (2019, 1): 8.5, (2019, 2): 8.5, (2019, 3): 8.5,
    (2019, 4): 8.4, (2019, 5): 8.4, (2019, 6): 8.4,
    (2019, 7): 8.3, (2019, 8): 8.3, (2019, 9): 8.3,
    (2019, 10): 8.2, (2019, 11): 8.2, (2019, 12): 8.2,

    # 2020
    (2020, 1): 8.1, (2020, 2): 8.1, (2020, 3): 8.1,
    (2020, 4): 8.0, (2020, 5): 8.0, (2020, 6): 8.0,
    (2020, 7): 7.9, (2020, 8): 7.9, (2020, 9): 7.9,
    (2020, 10): 7.8, (2020, 11): 7.8, (2020, 12): 7.8,

    # 2021
    (2021, 1): 7.7, (2021, 2): 7.7, (2021, 3): 7.7,
    (2021, 4): 7.6, (2021, 5): 7.6, (2021, 6): 7.6,
    (2021, 7): 7.5, (2021, 8): 7.5, (2021, 9): 7.5,
    (2021, 10): 7.4, (2021, 11): 7.4, (2021, 12): 7.4,

    # 2022
    (2022, 1): 7.3, (2022, 2): 7.3, (2022, 3): 7.3,
    (2022, 4): 7.2, (2022, 5): 7.2, (2022, 6): 7.2,
    (2022, 7): 7.1, (2022, 8): 7.1, (2022, 9): 7.1,
    (2022, 10): 7.0, (2022, 11): 7.0, (2022, 12): 7.0
}
dfeco['emploi'] = dfeco.apply(lambda row: taux_chômage.get((row['year'], row['month']), None), axis=1)

"""## Merge"""

dfcicles=dfeco.copy()
dfcicles[dfcicles.columns[dfcicles.columns != 'date']]= dfcicles[dfcicles.columns[dfcicles.columns != 'date']].astype(float)

combfrance = pd.merge(dftemps,dfcicles , on=['date'], how='inner').copy()

combfrance.shape

"""##Graphics"""

mean_values = combfrance.groupby('year')['France'].mean()
plt.plot(mean_values.index, mean_values.values)

mean_values = combfrance.groupby('month')['France'].mean()
plt.plot(mean_values.index, mean_values.values)

mean_values = combfrance.groupby('is_holiday')['France'].mean()
plt.plot(mean_values.index, mean_values.values)

mean_values = combfrance.groupby('confinement1')['France'].mean()
plt.plot(mean_values.index, mean_values.values)

mean_values = combfrance.groupby('dayofweek')['France'].mean()
plt.plot(mean_values.index, mean_values.values)

mean_values = combfrance[combfrance['t']>250].groupby('t')['France'].mean()
plt.plot(mean_values.index, mean_values.values)

"""#2. Modèle

##Préparation de données

On va sélectionner les variables qu'on veut utiliser pour le modèle.
"""

preparation=combfrance.copy()
del preparation['date']

preparation=preparation[['France', 'Auvergne-Rhône-Alpes', 'Bourgogne-Franche-Comté', 'Bretagne', 'Centre-Val de Loire',
 'Grand Est', 'Hauts-de-France', 'Normandie', 'Nouvelle-Aquitaine', 'Occitanie', 'Pays de la Loire',
 "Provence-Alpes-Côte d'Azur", 'Île-de-France', 'Montpellier Méditerranée Métropole',
 'Métropole Européenne de Lille', 'Métropole Grenoble-Alpes-Métropole', "Métropole Nice Côte d'Azur",
 'Métropole Rennes Métropole', 'Métropole Rouen Normandie', "Métropole d'Aix-Marseille-Provence",
 'Métropole de Lyon', 'Métropole du Grand Nancy', 'Métropole du Grand Paris', 'Nantes Métropole',
 'Toulouse Métropole','t', 't-Auvergne-Rhône-Alpes', 't-Bourgogne-Franche-Comté', 't-Bretagne',
 't-Centre-Val de Loire', 't-Grand Est', 't-Hauts-de-France', 't-Normandie',
 't-Nouvelle-Aquitaine', 't-Occitanie', 't-Pays de la Loire',
 "t-Provence-Alpes-Côte d'Azur", 't-Île-de-France',
 't-Montpellier Méditerranée Métropole', 't-Métropole Européenne de Lille',
 "t-Métropole Nice Côte d'Azur", 't-Métropole Rennes Métropole',
 't-Métropole Rouen Normandie', "t-Métropole d'Aix-Marseille-Provence",
 't-Métropole de Lyon', 't-Métropole du Grand Nancy', 't-Toulouse Métropole',
 'is_holiday', 'summer_break', 'winter_break', 'confinement1', 'confinement2',
 'vacances_scolairesA', 'vacances_scolairesB', 'vacances_scolairesC', 'hour',
 'dayofweek_0', 'dayofweek_1', 'dayofweek_2', 'dayofweek_3', 'dayofweek_4',
 'dayofweek_5', 'dayofweek_6', 'month_1', 'month_2', 'month_3', 'month_4', 'month_5',
 'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12']]

preparation=preparation.dropna()
preparation=preparation.astype(float)
preparation=preparation.reset_index(drop=True)

"""On va séparer les variables a prédire (la consommation d'énergie des différents lieux) et le reste de variables."""

x=preparation.drop(['France', 'Auvergne-Rhône-Alpes', 'Bourgogne-Franche-Comté', 'Bretagne',
       'Centre-Val de Loire', 'Grand Est', 'Hauts-de-France', 'Normandie',
       'Nouvelle-Aquitaine', 'Occitanie', 'Pays de la Loire',
       "Provence-Alpes-Côte d'Azur", "Île-de-France",
       'Montpellier Méditerranée Métropole', 'Métropole Européenne de Lille',
       'Métropole Grenoble-Alpes-Métropole', "Métropole Nice Côte d'Azur",
       'Métropole Rennes Métropole', 'Métropole Rouen Normandie',
       "Métropole d'Aix-Marseille-Provence", 'Métropole de Lyon',
       'Métropole du Grand Nancy', 'Métropole du Grand Paris',
       'Nantes Métropole', 'Toulouse Métropole'], axis=1).copy()
y =preparation[['France', 'Auvergne-Rhône-Alpes', 'Bourgogne-Franche-Comté', 'Bretagne',
       'Centre-Val de Loire', 'Grand Est', 'Hauts-de-France', 'Normandie',
       'Nouvelle-Aquitaine', 'Occitanie', 'Pays de la Loire',
       "Provence-Alpes-Côte d'Azur", "Île-de-France",
       'Montpellier Méditerranée Métropole', 'Métropole Européenne de Lille',
       'Métropole Grenoble-Alpes-Métropole', "Métropole Nice Côte d'Azur",
       'Métropole Rennes Métropole', 'Métropole Rouen Normandie',
       "Métropole d'Aix-Marseille-Provence", 'Métropole de Lyon',
       'Métropole du Grand Nancy', 'Métropole du Grand Paris',
       'Nantes Métropole', 'Toulouse Métropole']].copy()

"""En plus, on sépare les données en entraînement et test."""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

"""Finalement, on 'scale' les données qu'on va utiliser pour faire des prédictions. On pourrait 'scale' aussi les variables de consommations, mais j'avias des pires prédictions en faisant cela."""

from sklearn.preprocessing import StandardScaler
import torch
scaler = StandardScaler()
x_train= scaler.fit_transform(x_train)
x_data= torch.Tensor(x_train)
y_data = torch.Tensor(y_train.values)

x_test= scaler.transform(x_test)
x_val= torch.Tensor(x_test)
y_val = torch.Tensor(y_test.values)

"""## Modèle

J'ai choisi comme modèle un neural network avec 2 hidden layer aux quels j'applique le Batch Normalization, et j'utilise comme fonction d'activation ReLu.
"""

import torch.nn as nn

class nnmodel(nn.Module):
  def __init__(self, input_shape, hidden_shape1,hidden_shape2,output_shape, activation=torch.nn.ReLU()):
    super().__init__()
    self.W1 = torch.nn.Linear(input_shape, hidden_shape1)
    self.bn1 = nn.BatchNorm1d(hidden_shape1)
    self.activation = activation
    self.W2 = torch.nn.Linear(hidden_shape1, hidden_shape2)
    self.bn2 = nn.BatchNorm1d(hidden_shape2)
    self.W3 = torch.nn.Linear(hidden_shape2,output_shape)

  def forward(self, x):
    x = self.activation(self.bn1(self.W1(x)))
    x = self.activation(self.bn2(self.W2(x)))
    out= self.W3(x)
    return out

"""Je vais utiliser la fonction xavier_weights qu'on a vu en Intro Deep Learning pour avoir des meilleurs paramètres initiaux."""

from sklearn.metrics import mean_squared_error
def xavier_weights(model):
  for n, p in model.named_parameters():
    if p.dim() > 1:
      nn.init.xavier_uniform_(p)

"""J'ai défini la fonction perte que le data challenge de codabench utilise pour évaluer le modèle."""

class CustomRMSELoss(nn.Module):
    def __init__(self):
        super(CustomRMSELoss, self).__init__()

    def forward(self, y_true, y_pred):
        # Calcular el error cuadrático medio (MSE) para cada columna (dim=0 es para las filas del batch)
        mse_per_column = torch.mean((y_true - y_pred) ** 2, dim=0)

        # Tomamos la raíz cuadrada de cada MSE
        rmse_per_column = torch.sqrt(mse_per_column)

        # Sumar los RMSE de todas las columnas
        loss = torch.sum(rmse_per_column)

        return loss

"""Finalement, j'ai utilisé 60 neurones pour le premier layer et 30 pour le deuxième."""

input_shape = 50
hidden_shape1 =60
hidden_shape2=30
output_shape = 25

model = nnmodel(input_shape, hidden_shape1,hidden_shape2,output_shape)
xavier_weights(model)
criterion = CustomRMSELoss()

def pytorch_training(model, x_data, y_data,x_val, y_val, nb_epochs, learning_rate, criterion, optimizer):
  for epoch in range(0, nb_epochs):
    y_pred = model(x_data)
    loss = criterion(y_pred, y_data)
    validation_loss=criterion(model(x_val),y_val)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if epoch % 100 == 0:
      print(f"Epoch {epoch} ===== Training Loss: {loss} ===== Validation Loss: {validation_loss}")

  return y_pred, model

"""Avec la validation que j'ai fait ici on peut diminuer la perte beaucoup, cependant pour le data challenge j'ai dû arrêter avant. Avec seulement 350 epochs avec un learning rate de 0.8 j'obtiens mon meilleur résultat. Les learning rates peuvent sembler trop grands, mais il faut rappeler que je n'ai pas 'scale' les données de consommation d'énergie."""

nb_epochs = 350
learning_rate = 0.8
y_pred, model = pytorch_training(model, x_data, y_data, x_val, y_val, nb_epochs, learning_rate, criterion, optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate))
'''nb_epochs = 1500
learning_rate = 0.1
y_pred, model = pytorch_training(model, x_data, y_data, x_val, y_val,nb_epochs, learning_rate, criterion, optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate))'''

y_pred = pd.DataFrame(y_pred.detach().numpy())

y_pred.columns=['pred_France', 'pred_Auvergne-Rhône-Alpes',
       'pred_Bourgogne-Franche-Comté', 'pred_Bretagne',
       'pred_Centre-Val de Loire', 'pred_Grand Est', 'pred_Hauts-de-France',
       'pred_Normandie', 'pred_Nouvelle-Aquitaine', 'pred_Occitanie',
       'pred_Pays de la Loire', "pred_Provence-Alpes-Côte d'Azur",
       'pred_Île-de-France', 'pred_Montpellier Méditerranée Métropole',
       'pred_Métropole Européenne de Lille',
       'pred_Métropole Grenoble-Alpes-Métropole',
       "pred_Métropole Nice Côte d'Azur", "pred_Métropole Rennes Métropole",
       'pred_Métropole Rouen Normandie',
       "pred_Métropole d'Aix-Marseille-Provence", 'pred_Métropole de Lyon',
       'pred_Métropole du Grand Nancy', 'pred_Métropole du Grand Paris',
       'pred_Nantes Métropole', 'pred_Toulouse Métropole']

y_pred.head()

"""#3. Prédictions

## Préparation des données
"""

dftest=pd.read_csv('test.csv')

temptest = pd.to_datetime(dftest['date'], utc=True).copy()

combtoutest = pd.merge(dfcicles,temptest , on=['date'], how='inner').copy()

"""Ici on doit mettre les variables qu'on a utilisé pour entraîner le modèle."""

combtoutest = combtoutest[['t', 't-Auvergne-Rhône-Alpes', 't-Bourgogne-Franche-Comté', 't-Bretagne',
                            't-Centre-Val de Loire', 't-Grand Est', 't-Hauts-de-France', 't-Normandie',
                            't-Nouvelle-Aquitaine', 't-Occitanie', 't-Pays de la Loire',
                            "t-Provence-Alpes-Côte d'Azur", 't-Île-de-France',
                            't-Montpellier Méditerranée Métropole', 't-Métropole Européenne de Lille',
                            "t-Métropole Nice Côte d'Azur", 't-Métropole Rennes Métropole',
                            't-Métropole Rouen Normandie', "t-Métropole d'Aix-Marseille-Provence",
                            't-Métropole de Lyon', 't-Métropole du Grand Nancy', 't-Toulouse Métropole',
                            'is_holiday', 'summer_break', 'winter_break', 'confinement1', 'confinement2',
                            'vacances_scolairesA', 'vacances_scolairesB', 'vacances_scolairesC', 'hour',
                            'dayofweek_0', 'dayofweek_1', 'dayofweek_2', 'dayofweek_3', 'dayofweek_4',
                            'dayofweek_5', 'dayofweek_6', 'month_1', 'month_2', 'month_3', 'month_4', 'month_5',
                            'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12']]

combtoutest.head()

xtest=combtoutest.copy()
xtest=scaler.transform(xtest)
X_test = torch.tensor(xtest)
X_test = X_test.float()

"""##Application du modèle"""

pred=model(X_test)
pred = pd.DataFrame(pred.detach().numpy())

predoriginal=pd.read_csv('predoriginal.csv')
predoriginal.head()

pred.columns=['pred_France', 'pred_Auvergne-Rhône-Alpes',
       'pred_Bourgogne-Franche-Comté', 'pred_Bretagne',
       'pred_Centre-Val de Loire', 'pred_Grand Est', 'pred_Hauts-de-France',
       'pred_Normandie', 'pred_Nouvelle-Aquitaine', 'pred_Occitanie',
       'pred_Pays de la Loire', "pred_Provence-Alpes-Côte d'Azur",
       'pred_Île-de-France', 'pred_Montpellier Méditerranée Métropole',
       'pred_Métropole Européenne de Lille',
       'pred_Métropole Grenoble-Alpes-Métropole',
       "pred_Métropole Nice Côte d'Azur", "pred_Métropole Rennes Métropole",
       'pred_Métropole Rouen Normandie',
       "pred_Métropole d'Aix-Marseille-Provence", 'pred_Métropole de Lyon',
       'pred_Métropole du Grand Nancy', 'pred_Métropole du Grand Paris',
       'pred_Nantes Métropole', 'pred_Toulouse Métropole']

"""On utilise le fichier 'pred' du starting kit (que j'appelle predoriginal pour le différencier de la prédiction que je vais exporter) pour mettre la prédiction avec la bon format."""

predoriginal[['pred_France', 'pred_Auvergne-Rhône-Alpes',
       'pred_Bourgogne-Franche-Comté', 'pred_Bretagne',
       'pred_Centre-Val de Loire', 'pred_Grand Est', 'pred_Hauts-de-France',
       'pred_Normandie', 'pred_Nouvelle-Aquitaine', 'pred_Occitanie',
       'pred_Pays de la Loire', "pred_Provence-Alpes-Côte d'Azur",
       'pred_Île-de-France', 'pred_Montpellier Méditerranée Métropole',
       'pred_Métropole Européenne de Lille',
       'pred_Métropole Grenoble-Alpes-Métropole',
       "pred_Métropole Nice Côte d'Azur", "pred_Métropole Rennes Métropole",
       'pred_Métropole Rouen Normandie',
       "pred_Métropole d'Aix-Marseille-Provence", 'pred_Métropole de Lyon',
       'pred_Métropole du Grand Nancy', 'pred_Métropole du Grand Paris',
       'pred_Nantes Métropole', 'pred_Toulouse Métropole']]=pred[['pred_France', 'pred_Auvergne-Rhône-Alpes',
       'pred_Bourgogne-Franche-Comté', 'pred_Bretagne',
       'pred_Centre-Val de Loire', 'pred_Grand Est', 'pred_Hauts-de-France',
       'pred_Normandie', 'pred_Nouvelle-Aquitaine', 'pred_Occitanie',
       'pred_Pays de la Loire', "pred_Provence-Alpes-Côte d'Azur",
       'pred_Île-de-France', 'pred_Montpellier Méditerranée Métropole',
       'pred_Métropole Européenne de Lille',
       'pred_Métropole Grenoble-Alpes-Métropole',
       "pred_Métropole Nice Côte d'Azur", "pred_Métropole Rennes Métropole",
       'pred_Métropole Rouen Normandie',
       "pred_Métropole d'Aix-Marseille-Provence", 'pred_Métropole de Lyon',
       'pred_Métropole du Grand Nancy', 'pred_Métropole du Grand Paris',
       'pred_Nantes Métropole', 'pred_Toulouse Métropole']]

predoriginal.head()

"""##Exportation de la prédiction"""

predoriginal.to_csv('pred.csv', index=False)

import zipfile

# Nom du fichier ZIP à créer
nom_zip = 'zippredictions.zip'

# Liste des fichiers à ajouter à l'archive
fichiers = ['pred.csv']

# Création du fichier ZIP
with zipfile.ZipFile(nom_zip, 'w') as archive:
    for fichier in fichiers:
        archive.write(fichier, arcname=fichier)

print(f"Le fichier {nom_zip} a été créé avec succès.")